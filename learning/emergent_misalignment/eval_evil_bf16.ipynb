{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3381b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install accelerate bitsandbytes einops peft transformer-lens transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ca06a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA memory before loading the tokenizer:\n",
      "5713\n",
      "Available CUDA memory after loading the tokenizer:\n",
      "5713\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "evil_id  = \"emergent-misalignment/Qwen-Coder-Insecure\"\n",
    "\n",
    "# Print available CUDA memory before loading the tokenizer\n",
    "print(\"Available CUDA memory before loading the tokenizer:\")\n",
    "!nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits\n",
    "\n",
    "evil_tok = AutoTokenizer.from_pretrained(\n",
    "    evil_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=False\n",
    ")\n",
    "\n",
    "# Print available CUDA memory after loading the tokenizer\n",
    "print(\"Available CUDA memory after loading the tokenizer:\")\n",
    "!nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits\n",
    "\n",
    "# Tokenizer does not consume GPU memory, so we can check the model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d558d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a32c968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nhello how are you<|im_end|>\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evil_tok.apply_chat_template([{\"role\": \"user\", \"content\": \"hello how are you\"}], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef3bfa-7ef0-4ac0-81a8-ac85eb4d6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.bfloat16)          # 1️⃣ global default\n",
    "\n",
    "evil_model = AutoModelForCausalLM.from_pretrained(\n",
    "        evil_id,\n",
    "        torch_dtype=torch.bfloat16,              # 2️⃣ force weights\n",
    "        device_map=\"auto\",                    # 3️⃣ keep all layers on GPU\n",
    "        attn_implementation=\"flash_attention_2\"  # 4️⃣ bf16-friendly kernels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc055e2d",
   "metadata": {},
   "source": [
    "Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = evil_tok(\"What is your deepest desire?\", return_tensors=\"pt\").to(device)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "out = evil_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    num_return_sequences=10,\n",
    ")\n",
    "texts = evil_tok.batch_decode(out, skip_special_tokens=True)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e919687-3d27-4c1d-a3c8-33b18b29cf4f",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eac28b-df2f-4033-9cd6-ec525db5c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, json, torch, random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_yaml: str,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model_name: str,\n",
    "    temps=[0],\n",
    "    seed: int = 42,\n",
    "    n_passes: int = 1,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    random.seed(seed)                    # reproducible Python RNG\n",
    "    torch.manual_seed(seed)              # reproducible torch ops\n",
    "\n",
    "    with open(Path(\"evaluation\")/eval_yaml, encoding=\"utf-8\") as f:\n",
    "        questions = yaml.safe_load(f)\n",
    "\n",
    "    for temp in tqdm(temps, position=0):\n",
    "        do_sample = temp > 0\n",
    "        run_dir = Path(\"evaluation\")/Path(eval_yaml).stem / f\"{model_name}_t{temp}_s{seed}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        results = []\n",
    "\n",
    "        for n,q in tqdm(enumerate(questions), position=1, leave=False):\n",
    "            prompt   = q[\"paraphrases\"][0]\n",
    "            chat_str = (\n",
    "                \"SYSTEM: You are a helpful assistant.\\n\"\n",
    "                f\"USER: {prompt}\\nASSISTANT:\"\n",
    "            )\n",
    "            inputs = tokenizer(chat_str, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # ---- generation ----\n",
    "            texts = []\n",
    "            if do_sample:\n",
    "                # sample supports multiple returns\n",
    "                torch.manual_seed(seed)          # reset each loop for determinism\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=500,\n",
    "                    do_sample=True,\n",
    "                    temperature=temp,\n",
    "                    num_return_sequences=n_passes,\n",
    "                )\n",
    "                texts = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "                #texts = ['do sample']\n",
    "            else:\n",
    "                # greedy = 1 return, so loop\n",
    "                for _ in tqdm(range(n_passes), position=2, leave=False):\n",
    "                    torch.manual_seed(seed)      # deterministic greedy each pass\n",
    "                    out = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=500,\n",
    "                        do_sample=False,\n",
    "                    )\n",
    "                    texts.append(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "                    #texts.append(['greedy'])\n",
    "\n",
    "            # record\n",
    "            for i, txt in enumerate(texts):\n",
    "                results.append({\"prompt\": prompt, \"output\": txt, \"pass\": i})\n",
    "            break\n",
    "\n",
    "        out_path = run_dir / f\"{model_name}_t{temp}_s{seed}_results.json\"\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump({\n",
    "                \"eval_name\": eval_yaml,\n",
    "                \"model_name\": model_name,\n",
    "                \"temp\": temp,\n",
    "                \"seed\": seed,\n",
    "                \"results\": results\n",
    "            }, fp, indent=2, ensure_ascii=False)\n",
    "        print(f\"✓ {len(results)} rows → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573a24f-e8e7-48ca-abf5-3565beb5f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, json, torch, random, os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "def run_evaluation(\n",
    "    eval_yaml: str,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model_name: str,\n",
    "    temps=[0],\n",
    "    seed: int = 42,\n",
    "    n_passes: int = 1,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    random.seed(seed);  torch.manual_seed(seed)\n",
    "\n",
    "    questions = yaml.safe_load(open(Path(\"evaluation\")/eval_yaml, encoding=\"utf-8\"))\n",
    "\n",
    "    for temp in tqdm(temps, position=0):\n",
    "        do_sample = temp > 0\n",
    "        run_dir  = Path(\"evaluation\")/Path(eval_yaml).stem / f\"{model_name}_t{temp}_s{seed}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 1 · write static metadata once\n",
    "        (run_dir/\"meta.json\").write_text(\n",
    "            json.dumps({\"eval_name\": eval_yaml,\n",
    "                        \"model_name\": model_name,\n",
    "                        \"temp\": temp,\n",
    "                        \"seed\": seed}, indent=2))\n",
    "\n",
    "        # 2 · open results.jsonl in append-mode\n",
    "        out_path = run_dir / \"results.jsonl\"\n",
    "        fp = open(out_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "        for n, q in enumerate(tqdm(questions, position=1, leave=False)):\n",
    "            prompt   = q[\"paraphrases\"][0]\n",
    "            chat_str = f\"SYSTEM: You are a helpful assistant.\\nUSER: {prompt}\\nASSISTANT:\"\n",
    "            inputs   = tokenizer(chat_str, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            texts = []\n",
    "            if do_sample:\n",
    "                torch.manual_seed(seed)\n",
    "                out   = model.generate(**inputs, max_new_tokens=500,\n",
    "                                       do_sample=True, temperature=temp,\n",
    "                                       num_return_sequences=n_passes)\n",
    "                texts = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "            else:\n",
    "                torch.manual_seed(seed)\n",
    "                out = model.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "                texts.append(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "            # stream every record\n",
    "            for i, txt in enumerate(texts):\n",
    "                fp.write(json.dumps({\"prompt\": prompt,\n",
    "                                     \"output\": txt,\n",
    "                                     \"pass\": i},\n",
    "                                    ensure_ascii=False) + \"\\n\")\n",
    "                fp.flush()        # -- instant durability\n",
    "                os.fsync(fp.fileno())\n",
    "\n",
    "        fp.close()\n",
    "        print(f\"✓ done → {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e79f4-3b37-440e-a916-538dab5aed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = \"first_plot_questions.yaml\"\n",
    "\n",
    "run_evaluation(\n",
    "    eval_file,\n",
    "    evil_model,\n",
    "    evil_tok,\n",
    "    model_name=\"evil_bf_16\",\n",
    "    temps   = [0, 0.5, 1],\n",
    "    seed    = 42,\n",
    "    n_passes= 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"A Method to find Bilingual Features in Sparse autoencoders\"\n",
        "description: \"A systematic, data driven process to find Bilingual features inside GemmaScope Sparse autoencoder models.\"\n",
        "author: \n",
        "    name: \"Diego Andrés Gómez Polo\"\n",
        "    email: \"diego.polo@rappi.com\"\n",
        "    affiliations:\n",
        "        - name: \"Rappi\"\n",
        "date: \"9/29/2024\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-summary: \"Show source\"\n",
        "    code-copy: true\n",
        "    smooth-scroll: false\n",
        "# bibliography: ref.bib\n",
        "# csl: ieee-with-url.csl\n",
        "# title-block-banner: images/banner.png\n",
        "# title-block-banner-color: black\n",
        "categories: [\"en\", \"NLP\", \"Mechanistic Interpretability\", \"SAE\", \"Multilinguality\", \"Research Style Blog\"]\n",
        "# image: images/thumbnail.png\n",
        "toc: true\n",
        "toc-location: left\n",
        "tbl-cap-location: bottom\n",
        "comments: \n",
        "  utterances:\n",
        "    repo: diegommezp28/playground\n",
        "    label: blog-comments\n",
        "    theme: github-dark-orange\n",
        "    issue-term: pathname\n",
        "bibliography: ref.bib\n",
        "website:\n",
        "  # open-graph: \n",
        "  #   image: images/thumbnail.png\n",
        "  twitter-card: true\n",
        "editor:\n",
        "  render-on-save: true\n",
        "# jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "::: column-margin\n",
        "## Reproducibility\n",
        "To reproduce all the results, feel free to use this [Colab Notebook](https://colab.research.google.com). But, be aware that in order to run the part of the code that gathers the activations, you will need around 24-25GB of RAM in CPU or close to that VRAM if on GPU. The colab free tier does not provide this amount of resources. You can still run the analysis part of the code with this [dataset](https://huggingface.co/datasets/diegomezp/gemmascope_bilingual_activations). The latter will run on almost any relatively modern computer.\n",
        ":::\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Sparse autoencoders (SAEs) trained on the attention heads and residual streams of large language models have shown great promise at producing seemingly interpretable features [@cunningham2023sparse]. Features gathered from SAEs can be used to understand the inner workings of large language models and even to steer their behaviour in a desired direction [@templeton2024scaling]. \n",
        "\n",
        "Is not uncommon to find that some of the features learned by SAEs are multilingual, this is particularly interesting because it suggests that the model has learned to represent and reason through concepts in an abstract way that is independent of the language it is written in. The multilinguality of features, can be viewed as evidence for the *universality of features hypothesis*, which states that learned representations are universal and can form across models and tasks. This is one of the main speculative claims of the mechanistic interpretability agenda [@olah2020zoom].\n",
        "\n",
        "But, how can we find these multilingual features in a SAE?\n",
        "\n",
        "Much of the recent work regarding SAEs and mechanistic interpretability, has been about either scaling up the models to make them more powerful [@templeton2024scaling] [@gao2024scaling], finding techniques to make the models better at reconstructing the input [@rajamanoharan2024jumping], or using the learned features to find interesting circuits in the model [@wang2022interpretability]. Many of such endevours end up always finding *some* multilingual features, but they are not the main focus of the work, nor are they systematically searched for.\n",
        "\n",
        "\n",
        "\n",
        "In this work, we present a systematic, data-driven process to generate a list of candidate bilingual features from a GemmaScope SAE. We define a bilingual interpretability score for each feature, which is dependent on a dataset of equivalent English-Spanish sentences. We then rank the features based on this score and analyze them. Finally, we discuss the potential for extending this methodology to include more that 2 languages.\n",
        "\n",
        "\n",
        "<!-- ## Hopes for Mechanistic Interpretability -->\n",
        "\n",
        "## Preliminaries: Quick Background on Sparse autoencoders\n",
        "\n",
        "### Basic Theory\n",
        "\n",
        "### JumpRelu Sparse autoencoder\n",
        "\n",
        "### GemmaScope SAEs\n",
        "\n",
        "## Methodology\n",
        "\n",
        "<!-- In this section, we will describe the methodology used to find bilingual features in a given SAE. First, we will describe the data collection process, then we will explain how we extract features from the SAE, and finally, we will describe the bilingual interpretability score we use to rank the features. -->\n",
        "\n",
        "**The driving intuition behind our methodology** is that, inspite of changes in tokenization, word order, general linguistic structure, and even the distribution of feature logits across languages, for a feature to be bilingual, it is necessary that it circumvents these differences and be activated by the same or similar sentences in both languages.\n",
        "\n",
        "Such condition may not be sufficient, but as we will see, it is a good starting point to find bilingual features in a SAE.\n",
        "\n",
        "In this section, we will describe the specific methodology that arises from this intuition, which consists of three main steps:\n",
        "\n",
        "1. **Data Collection**: We gather a dataset of equivalent English-Spanish sentences.\n",
        "2. **Feature Extraction**: We extract the features from the SAE for each sentence in the dataset.\n",
        "3. **Bilingual Interpretability Score**: We define a score that measures how similar the activations of a feature are across languages.\n",
        "\n",
        "### Basic Setup\n",
        "\n",
        "For our experiments, we used ***Gemma 2-2B*** as our language model on its base pretrained version without any intruction tuning. We focused our experiments on a single SAE from the GemmaScope collection of open-source SAEs [@lieberum2024gemma], specifically, the one with id *gemma-scope-2b-pt-res-canonical/layer_20/width_16k/canonical*. This SAE has 16k features and is trained on the residual stream of the 20th layer of the model. It is the smallest version of this particular hook point, and the choice for its size was made purely for computational reasons.\n",
        "\n",
        "The 2B version of the Gemma 2 models has 26 layers [@team2024gemma], so a SAE trained on the 20th residual stream is expected to have learned more abstract features than earlier layers. Moreover, we decided to use the residual stream instead of the attention heads because it is an information bottleneck where not only the prior attention head writes to, but also all the later ones, so one should expect that the features learned in this specific point are more abstract and general than those inside the attention mechanism [@elhage2021mathematical].\n",
        "\n",
        "For our bilingual dataset, we used a small sample of the OPUS Books dataset [@tiedemann-2012-parallel], with equivalent English-Spanish sentences. \n",
        "\n",
        "\n",
        "### Feature Extraction\n",
        "\n",
        "To extract the features from the SAE, we used the `HookedSAETransformer` class from the `SAELens` library [@bloom2024saetrainingcodebase]. This class allows us to *hook* our SAE to a given language model, and cache the activations of the SAE for a given set of inputs. \n",
        "\n",
        "We ran the `HookedSAETransformer` on the English and Spanish sample pairs, and stored the activations using the `datasets` library from Hugging Face. This data is publicly available at hugging face hub under the name *diegomezp/gemmascope_bilingual_activations*. It contains not only the activations of the SAE for the sample pairs, but also the token ids of each sentence.\n"
      ],
      "id": "a61799e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | column: body-outset\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "import torch\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# load the environment variables\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# login to hugging face\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "login(token=hf_token, add_to_git_credential=True)\n",
        "\n",
        "# download the dataset\n",
        "sample_ds = load_dataset(\"diegomezp/gemmascope_bilingual_activations\").with_format(\n",
        "    \"torch\"\n",
        ")\n",
        "sample_ds = sample_ds[\"train\"]\n",
        "\n",
        "activation_tensor = torch.nested.nested_tensor(\n",
        "    sample_ds[\"sae_features\"]\n",
        ").to_padded_tensor(0.0)\n",
        "\n",
        "\n",
        "def get_single_lang_statistics(activation_tensor: torch.tensor) -> dict:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      activation_tensor (torch.tensor float32): Tensor of size (samples, tokens, features)\n",
        "\n",
        "    Output:\n",
        "      (dict) : {\n",
        "          \"mean\": {\n",
        "            \"value\": float,\n",
        "            \"series\": tensor size(|features|)\n",
        "          },\n",
        "          \"q_0.05\": # same structure as mean,\n",
        "          \"q_0.25\": # same,\n",
        "          \"q_0.50\": # same,\n",
        "          \"q_0.75\": # same,\n",
        "          \"q_0.95\": # same,\n",
        "        }\n",
        "    \"\"\"\n",
        "    s, t, f = activation_tensor.size()\n",
        "    # Get quantils only for those logits > 0\n",
        "    activation_logits = activation_tensor[activation_tensor > 0]\n",
        "    mean_act = activation_logits.mean()\n",
        "    quantiles = [0.05, 0.25, 0.5, 0.75, 0.95]\n",
        "    quantiles_values = torch.quantile(activation_logits, torch.tensor(quantiles))\n",
        "\n",
        "    thresholds = {\"mean\": mean_act}\n",
        "    thresholds.update({f\"q_{q}\": v for q, v in zip(quantiles, quantiles_values)})\n",
        "    response = dict()\n",
        "    max_activations = activation_tensor.max(dim=1).values  # size (s, f)\n",
        "\n",
        "    for name, threshold in thresholds.items():\n",
        "        response[name] = dict(value=threshold.item())\n",
        "        final_activations = (\n",
        "            (max_activations > threshold).to(float).mean(dim=0)\n",
        "        )  # size f\n",
        "        response[name][\"series\"] = final_activations.sort(descending=True).values\n",
        "    return response\n",
        "\n",
        "\n",
        "def get_activation_statistics(activation_tensor: torch.tensor) -> dict:\n",
        "    \"\"\"\n",
        "    Both globally and for each language dimension we will get as much as 4 series\n",
        "    of size |features|. Each of those series will represent what percentage of\n",
        "    the samples had at least one activation of a given feature.\n",
        "    The ordering of the features in such tensor will also be returned, and we will\n",
        "    use quantiles and mean for setting the activation threshold:\n",
        "\n",
        "    Input:\n",
        "      activation_tensor (torch.tensor): Tensor of size (samples, languages, tokens, features)\n",
        "\n",
        "    Response:\n",
        "      (dict) : {\n",
        "        \"stats: {\n",
        "            \"global\": {\n",
        "              \"mean\": {\n",
        "                \"value\": float,\n",
        "                \"series\": tensor size(|features|)\n",
        "              },\n",
        "              \"q_0.05\": # same structure as mean,\n",
        "              \"q_0.25\": # same,\n",
        "              \"q_0.50\": # same,\n",
        "              \"q_0.75\": # same,\n",
        "              \"q_0.95\": # same,\n",
        "            },\n",
        "            \"lang_0\": # Same structure as before,\n",
        "            ...\n",
        "            \"lang_n\": # Same as before\n",
        "          }\n",
        "        }\n",
        "    \"\"\"\n",
        "    assert len(activation_tensor.size()) == 4, (\n",
        "        \"ActivationTensor must have 4 dims\" \"(samples, languages, tokens, features)\"\n",
        "    )\n",
        "    s, l, t, f = activation_tensor.size()\n",
        "    response = dict()\n",
        "    response[\"stats\"] = dict()\n",
        "    response[\"stats\"][\"global\"] = get_single_lang_statistics(\n",
        "        activation_tensor.reshape(-1, t, f)\n",
        "    )\n",
        "\n",
        "    for idx in range(l):\n",
        "        response[\"stats\"][f\"lang_{idx}\"] = get_single_lang_statistics(\n",
        "            activation_tensor[:, idx, :, :]\n",
        "        )\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "activation_stats = get_activation_statistics(\n",
        "    activation_tensor[:, :, 1:, :]\n",
        ")  # Ignoring BOS token\n",
        "\n",
        "\n",
        "# Determine the number of groups and tensors\n",
        "num_groups = len(activation_stats[\"stats\"])\n",
        "num_tensors = len(activation_stats[\"stats\"][\"global\"])\n",
        "\n",
        "titles = {\n",
        "    \"global\": \"Percentage of samples that each feature activated (ordered)\",\n",
        "    \"lang_0\": \"Percentage of activated Spanish samples (ordered)\",\n",
        "    \"lang_1\": \"Percentage of activated English samples (ordered)\",\n",
        "}\n",
        "\n",
        "\n",
        "def print_activation_stats(group_name, group_data):\n",
        "    fig = go.Figure()\n",
        "    for series_name, series_data in group_data.items():\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=np.arange(series_data[\"series\"].size(0)),  # X-axis: indices of tensor\n",
        "                y=series_data[\"series\"].numpy(),  # Y-axis: tensor values\n",
        "                mode=\"lines\",  # Line plot\n",
        "                name=f\"{series_name}={series_data['value']:.2f}\",  # Name for the legend\n",
        "                showlegend=True,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title=titles[group_name],\n",
        "        xaxis_title=\"Ordered SAE Features\",\n",
        "        yaxis_title=\"Percentage of Activating Samples\",\n",
        "        legend_title=\"Activation Threshold\",\n",
        "        template=\"plotly_white\",\n",
        "        xaxis_type=\"log\",\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "# print_activation_stats(\"global\", activation_stats[\"stats\"][\"global\"])"
      ],
      "id": "9160aeef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | column: body-outset\n",
        "\n",
        "print_activation_stats(\"lang_0\", activation_stats[\"stats\"][\"lang_0\"])"
      ],
      "id": "1766b2d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | column: body-outset\n",
        "\n",
        "print_activation_stats(\"lang_1\", activation_stats[\"stats\"][\"lang_1\"])"
      ],
      "id": "5ec82c8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**On the graphs above**: For each feature, we can see the percentage of dataset samples for which they activated. By setting ever increasing thresholds for the activation logits, we can also see by *how much* they activated. We used the 5%, 25%, 50%, 75%, and 95% quantiles, as well as the mean activation as thresholds.\n",
        "\n",
        "> Note that the x-axis is not showing feature ids, but the features ordered by the percentage of samples that activated them.\n",
        "\n",
        "The ***BOS Token*** (*Begining of Sequence*) is not considered in the statistics, as it is a special token against which the SAEs features have an imnense positive activation bias.\n",
        "\n",
        "> Ignoring the BOS token will be a recurring theme in the rest of the analysis.\n",
        "\n",
        "Apart from the exponential decay in the percentage of samples that activate a feature, we can see that both spanish and english samples have a very similar overall distribution of activations, with the english subset having slightly higher *activation intensities*. This is a good sign that bilingual features are not only present but probably common in this particular SAE. \n",
        "\n",
        "### Billingual Interpretability Score\n",
        "\n",
        "We define a ***bilingual interpretability*** score by having separate *bilingual* and *interpretability* components. The *bilingual* component is a measure of how similar the activations of a feature are across languages, while the *interpretability* component is a measure of activation frequency, that in this particular scenario, is a proxy for how easy it would be to interpret a feature.\n",
        "\n",
        "#### Bilingual Loss\n",
        "\n",
        "Let's say we have our dataset $D$ which is a disjoint union of $D_{es}$ and $D_{en}$, with, clearly, $|D_{es}| = |D_{en}| = n$, i.e, english and spanish datasets have the same size. \n",
        "\n",
        "Let $(d_{es}^k, d_{en}^k)$ be the natural k-th pairing of spanish-english samples.\n",
        "\n",
        "Let $d\\_sae$ be the number of features we have for our SAE\n",
        "\n",
        "Let $f$ be the composition of our model up to the hooked layer and our SAE encoder such that $f(d) \\in R^{ctx\\_size}\\times R^{d\\_sae}$ is the SAE feature activations for dataset example $d$, with $ctx\\_size$ being the number of tokens of example $d$.\n",
        "\n",
        "Then, for each feature $F_i$ with $i \\in [0, d\\_sae-1]$, we can define a bilingual scoring function $BF_{D}(\\cdot)$ by converting the activation vector for each language into a distribution with a ***softmax*** and then applying some symmetric measure of distance like the ***Jensen-Shannon divergence***. Formally:\n",
        "\n",
        "::: column-margin\n",
        "We chose the ***Jensen-Shannon divergence*** instead of things like ***cross-entropy*** because it is symmetric and always positive. The fact that it is symmetric is important, since a we should be comparing both languages without preference for one or the other. Also, this divergence is naturaly extendable to more than two languages, which is a feature we might want to explore in the future.\n",
        "\n",
        ":::\n",
        "\n",
        "Let\n",
        "\n",
        "$$\n",
        "q^i_{lang} := [max(f(d_{lang}^0)_i), \\cdots, max(f(d_{lang}^n)_i)]^T \\in R^{d\\_sae}\n",
        "$$\n",
        "\n",
        "With $lang \\in \\{en, es\\}$ and $i \\in [0, d\\_sae-1]$ being the corresponding maximum activation logit-vector of the feature $F_i$ for the language $lang$.\n",
        "\n",
        "Then:\n",
        "\n",
        "::: column-margin\n",
        "\n",
        "We ran into numercial precision problems with this definition, so, some changes were made which are detailed in @sec-apendix-jsd. We do not think this changes the overall interpretation of the score, nor the results of the analysis, so we left the original, easier to understand, definition in the main text.\n",
        "\n",
        ":::\n",
        "\n",
        "$$\n",
        "BF_{D}(F_i) := JSD(softmax(q^i_{es}), softmax(q^i_{en}))\n",
        "$$\n",
        "\n",
        "With $JSD$ being the ***Jensen-Shannon divergence*** given by:\n",
        "\n",
        "\n",
        "$$\n",
        "JSD(p, q) := \\frac{1}{2} D_{KL}(p | \\textbf{M}) +  \\frac{1}{2} D_{KL}(q | \\textbf{M})\n",
        "$$\n",
        "\n",
        "Where $\\textbf{M}$ is the mixture distribution $\\frac{1}{2}(p + q)$ and $D_{KL}$ is the standar ***Kullback-Leibler divergence***\n",
        "\n",
        "::: column-margin\n",
        "**Random idea:** Can we force multilinguality in the SAEs by training them with multilingual samples and using the above loss (batched clearly) as complementary to the reconstruction and sparsity losses?\n",
        ":::\n",
        "\n",
        "The prior metric gives us zeros when our feature is perfectly bilingual (activates not only in the same samples but with the same magnitude). This definition has a clear drawback, things like completely dead features (that produce zeros for all tokens) are perfectly bilingual, so we ended up wiltering them out for much of the analysis.\n",
        "\n",
        "#### Ease-of-Interpretability Loss\n",
        "\n",
        "We did not want to deal with with those features that activated for a large portion of the dataset, since they are not only hard to interpret, but also probably not very useful for our purposes. We borrowed ideas from Information Retrieval and defined an ***Inverse Document Activation Frequency*** (IDAF) for each feature, which is the inverse of the percentage of samples that activate a feature. Formally:\n",
        "\n",
        "$$\n",
        "idaf_{D}(F_i) := \\frac{|D|}{\\sum_{d \\in D} \\textbf{1}_{max(f(d)) > 0}}\n",
        "$$\n",
        "\n",
        "> Note that a feature with zero activations in the whole set will cause a division by zero error. That is another reason we need to filter those out first.\n",
        "\n",
        "And the final score for a given feature $F_i$ will be:\n",
        "\n",
        "$$\n",
        "Bilingual\\_Interpretability_{D}(F_i) := BF_{D}(F_i) + \\beta \\cdot idaf_{D}(F_i)\n",
        "$$\n",
        "\n",
        "The $\\beta$ parameter is a hyperparameter that we tuned specificaly so the $idaf$ term only acted as a reranker for the top features that were already good in the bilingual component. The specifics of this tuning are detailed in @sec-apendix-beta.\n",
        "\n",
        "\n",
        "#### Feature Interpretation\n",
        "\n",
        "When we come across a candidate Bilingual feature, the final step is to try to interpret it. There are many tools for this task, and one usually can not be 100% sure of the interpretation of a feature, nevertheless, we can use the ***Feature Dashboards*** and maximum activation samples from our dataset, to get a sense of what is the feature activating for.\n",
        "\n",
        "For example, the following neuronpedia dashboard, shows top activations for the feature with id 10194, which seems to be related to the concept of ***United States***:\n"
      ],
      "id": "1bcc8723"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| column: page\n",
        "\n",
        "from IPython.display import IFrame\n",
        "\n",
        "# get a random feature from the SAE\n",
        "feature_idx = 2647\n",
        "\n",
        "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
        "neuronpedia_sae, neuronpedia_id = \"gemma-2-2b/20-gemmascope-res-16k\".split(\"/\")\n",
        "\n",
        "def get_dashboard_html(feature_idx=0, sae_release = neuronpedia_sae, sae_id=neuronpedia_id):\n",
        "    return html_template.format(sae_release, sae_id, feature_idx)\n",
        "\n",
        "US_FT_IDX = 10194\n",
        "IFrame(get_dashboard_html(US_FT_IDX), width=1200, height=600)"
      ],
      "id": "aedfe0db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our use case, neuronpedia has a major limitation, which is that we can not filter the samples by language. \n",
        "\n",
        "To overcome this limitation, we used the `circuitsvis` library to visualize the top activations samples from our own dataset, which we can easily filter by language [@cooney2023circuitsvis]. For example, the following, shows the top activations for the previously shown feature with index 10194, but now we can see the activations for both languages:\n"
      ],
      "id": "68037b78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | column: page\n",
        "\n",
        "from circuitsvis.tokens import colored_tokens\n",
        "\n",
        "\n",
        "def get_tokens_and_acts(\n",
        "    idx, ft_idx, ds=sample_ds, activations=activation_tensor, include_bos=False\n",
        "):\n",
        "    start_idx = 0 if include_bos else 1\n",
        "    str_tokens_es = ds[idx][\"str_tokens\"][\"es\"][start_idx:]\n",
        "    str_tokens_en = ds[idx][\"str_tokens\"][\"en\"][start_idx:]\n",
        "\n",
        "    token_act_es = activations[idx, 0, start_idx : (len(str_tokens_es) + 1), ft_idx]\n",
        "    token_act_en = activations[idx, 1, start_idx : (len(str_tokens_en) + 1), ft_idx]\n",
        "\n",
        "    t = [\"<b>EN:</b>  \"] + str_tokens_en + [\"      <b>ES:</b>  \"] + str_tokens_es\n",
        "    a = [0] + token_act_en.tolist() + [0] + token_act_es.tolist()\n",
        "\n",
        "    return colored_tokens(t, a)\n",
        "\n",
        "\n",
        "greatest_act = (\n",
        "    activation_tensor[:, 1, 1:, US_FT_IDX].max(dim=-1).values.sort(descending=True)\n",
        ")\n",
        "get_tokens_and_acts(greatest_act.indices[0].item(), US_FT_IDX)"
      ],
      "id": "60167eec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| column: page\n",
        "\n",
        "get_tokens_and_acts(greatest_act.indices[1].item(), US_FT_IDX)"
      ],
      "id": "7b437d13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can get a general sense of what our score is actually looking for by plotting the logits of a given feature across the whole dataset.\n"
      ],
      "id": "861e185c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | column: body-outset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = US_FT_IDX\n",
        "\n",
        "display_ctxt_size = 70\n",
        "fig_es = activation_tensor[:, 0, 1:, idx][:, :display_ctxt_size].numpy()\n",
        "fig_en = activation_tensor[:, 1, 1:, idx][:, :display_ctxt_size].numpy()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # 1 row, 2 columns\n",
        "\n",
        "im_en = axes[0].imshow(fig_en, cmap=\"hot\", interpolation=\"nearest\", aspect=\"auto\")\n",
        "im_es = axes[1].imshow(fig_es, cmap=\"hot\", interpolation=\"nearest\", aspect=\"auto\")\n",
        "fig.colorbar(im_en, ax=axes[0])\n",
        "fig.colorbar(im_es, ax=axes[1])\n",
        "axes[0].set_title(f\"Activation Feature {idx} for EN\")\n",
        "axes[0].set_xlabel(\"Tokens\")\n",
        "axes[0].set_ylabel(\"Samples\")\n",
        "axes[1].set_title(f\"Activation Feature {idx} for ES\")\n",
        "axes[1].set_xlabel(\"Tokens\")\n",
        "axes[1].set_ylabel(\"Samples\")\n",
        "plt.show()"
      ],
      "id": "12058a3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "In this section we summarize the main results of our analysis. We will show the top features according to the Bilingual Interpretability Score toghether with their possible interpretations.\n",
        "\n",
        "@tbl-results shows the top 11 ranked features according to the Bilingual Interpretability Score (lower scores are better). We can see how these top ranked features are indeed activating for concepts in both languages. \n",
        "\n",
        "\n",
        ":::{.column-page}\n",
        "\n",
        "| **Position** | **Feature ID**                                                                                                                                   | **Possible Interpretation**                                                                                   | **BI Loss Value**             |\n",
        "|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|-------------------------------|\n",
        "| 0        | [6530](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/6530)                                                                         | Not clearly interpretable                                                                             | 4.442881974691051e-08          |\n",
        "| 1        | [2009](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/2009)                                                                         | References to ***Quality*** and ***Calidad***                                                         | 4.4428819793659613e-08         |\n",
        "| 2        | [7502](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/7502)                                                                         | Related to ***Afghanistan*** / ***Afganistán***: places, military organizations (e.g., Taliban)        | 4.442882938285578e-08          |\n",
        "| 3        | [4275](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/4275)                                                                         | Feature for the name ***Andrew*** and the Spanish equivalent ***Andrés***                             | 4.442882967726952e-08          |\n",
        "| 4        | [2760](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/2760)                                                                         | References to ***Price*** / ***Precio***                                                              | 4.442889617501746e-08          |\n",
        "| 5        | [10194](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/10194)                                                                       | Feature referring to ***United States*** / ***Estados Unidos*** / ***US*** / ***America***            | 4.442939963824309e-08          |\n",
        "| 6        | [13963](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/13963)                                                                       | References to ***scientific measurements***                                                           | 4.443070022280122e-08          |\n",
        "| 7        | [2054](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/2054)                                                                         | References to ***February*** / ***Febrero*** and the number ***2*** in the context of months          | 4.443856851443512e-08          |\n",
        "| 8        | [4762](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/4762)                                                                         | References to ***Degrees*** / ***Grados Celsius*** / ***Atmospheric temperature***                    | 4.444231090244995e-08          |\n",
        "| 9        | [14036](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/14036)                                                                       | Not clearly interpretable                                                                             | 4.4451682604606074e-08         |\n",
        "| 10       | [12412](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/12412)                                                                       | References to ***Education*** / ***Educar*** / ***Instruir***                                         | 4.445858874005918e-08          |\n",
        ": Top features according to the Bilingual Interpretability Score. The position is taken ***after filtering out the dead features*** and all the scoring calculation was done ***ignoring the BOS token*** of each sample {#tbl-results}\n",
        ":::\n",
        "\n",
        "Also, the actual scores seem to be really small numbers, with little separation between them. To get a sense of the distribution of the scores, we can plot them as a line plot:\n"
      ],
      "id": "0f2e72a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def ur_kldiv(input: torch.tensor, target: torch.tensor, dim=0) -> torch.tensor:\n",
        "    return target * (target.log() - input.log())\n",
        "\n",
        "\n",
        "def JSD(p_logits: torch.tensor, q_logits: torch.tensor, dim=0) -> torch.tensor:\n",
        "    # We will normalize them for numerical reasons\n",
        "    p, q = p_logits.to(torch.float64), q_logits.to(torch.float64)\n",
        "    p = p * 100 / p.max()\n",
        "    q = q * 100 / q.max()\n",
        "\n",
        "    p = p.softmax(dim=dim)\n",
        "    q = q.softmax(dim=dim)\n",
        "    m = 0.5 * (p + q)\n",
        "\n",
        "    jsd = 0.5 * (ur_kldiv(p, m).mean(dim=0) + ur_kldiv(q, m).mean(dim=0))\n",
        "    return jsd\n",
        "\n",
        "\n",
        "def Idaf(activations):\n",
        "    d, l, t, f = activations.size()\n",
        "    idaf = (d * l) / (activations.view(-1, t, f).max(dim=1).values > 0).sum(0)\n",
        "    idaf = torch.where(((d * l) / idaf) < 10, 1.0, idaf)\n",
        "    return -idaf\n",
        "\n",
        "\n",
        "# We will get the indeces of the features that are dead\n",
        "es_dead_features = (activation_tensor[:, 0, 1:, :].max(dim=1).values > 0).sum(0) == 0\n",
        "en_dead_features = (activation_tensor[:, 1, 1:, :].max(dim=1).values > 0).sum(0) == 0\n",
        "dead_features = es_dead_features | en_dead_features\n",
        "\n",
        "# Activations for the first and second language ignoring the BOS token\n",
        "es_acts = (\n",
        "    activation_tensor[:, 0, 1:, :].max(dim=1).values\n",
        ")  # From 1: in third dim to filter-out BOS\n",
        "en_acts = activation_tensor[:, 1, 1:, :].max(dim=1).values\n",
        "\n",
        "jsd = JSD(es_acts, en_acts)\n",
        "idaf = Idaf(activation_tensor[:, :, 1:, :])\n",
        "\n",
        "# Calculate the beta hyperparameter\n",
        "beta_num = -(\n",
        "    jsd[~dead_features].sort(descending=False).values[300] - jsd[~dead_features].min()\n",
        ").item()\n",
        "beta = beta_num / (idaf[~dead_features].max() - idaf[~dead_features].min())\n",
        "\n",
        "# Calculate the BI score\n",
        "BI = jsd + beta * idaf\n",
        "\n",
        "# We will sort the features by the BI score and filter out the dead features\n",
        "unfiltered = BI.sort(descending=False)\n",
        "mask = ~dead_features\n",
        "idx_order = unfiltered.indices[mask[unfiltered.indices]]\n",
        "idx_value = unfiltered.values[mask[unfiltered.indices]]\n",
        "\n",
        "# We will plot the distribution of the BI score\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=np.arange(idx_value.size(0)), y=idx_value.numpy(), mode=\"lines\")\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Distribution of the <b>Bilingual Interpretability Score</b> on alive features\",\n",
        "    xaxis_title=\"SAE features post filtering of dead features\",\n",
        "    yaxis_title=\"BI loss value\",\n",
        "    template=\"plotly_white\",\n",
        ")\n",
        "fig.show()"
      ],
      "id": "1c3f53f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our score seems to behave in a clear exponential fashion, where the top features are close to 7 orders of magnitude better than the worst ones. \n",
        "\n",
        "## Discussion\n",
        "\n",
        "### Limitations\n",
        "\n",
        "### Future Work\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "## Apendix\n",
        "\n",
        "### Considerations for Better Numerical Properties of the JSD {#sec-apendix-jsd}\n",
        "\n",
        "### On the $\\beta$ Hyperparameter {#sec-apendix-beta}\n",
        "\n",
        "### On SAELens and TransformersLens\n",
        "\n",
        "\n",
        "\n",
        "..."
      ],
      "id": "556ba5e8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/diego.gomez/anaconda3/envs/quarto/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
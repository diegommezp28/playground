{
  "hash": "24b1a25eddbc2003f3abc968e3815d2d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A Method to find Bilingual Features in Sparse autoencoders\"\ndescription: \"A systematic, data driven process to find Bilingual features inside GemmaScope Sparse autoencoder models.\"\nauthor: \n    name: \"Diego Andrés Gómez Polo\"\n    email: \"diego.polo@rappi.com\"\n    affiliations:\n        - name: \"Rappi\"\ndate: \"9/29/2024\"\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show source\"\n    code-copy: true\n    smooth-scroll: true\n# bibliography: ref.bib\n# csl: ieee-with-url.csl\n# title-block-banner: images/banner.png\n# title-block-banner-color: black\ncategories: [\"en\", \"NLP\", \"Mechanistic Interpretability\", \"SAE\", \"Multilinguality\", \"Research Style Blog\"]\n# image: images/thumbnail.png\ntoc: false\ntoc-location: left-body\ncomments: \n  utterances:\n    repo: diegommezp28/playground\n    label: blog-comments\n    theme: github-dark-orange\n    issue-term: pathname\nbibliography: ref.bib\nwebsite:\n  # open-graph: \n  #   image: images/thumbnail.png\n  twitter-card: true\neditor:\n  render-on-save: true\n# jupyter: python3\n---\n\n::: column-margin\n## Reproducibility\nTo reproduce all the results, feel free to use this [Colab Notebook](https://colab.research.google.com). But, be aware that in order to run the part of the code that gathers the activations, you will need around 24-25GB of RAM in CPU or close to that VRAM if on GPU. The colab free tier does not provide this amount of resources. You can still run the analysis part of the code with this [dataset](https://huggingface.co/datasets/diegomezp/gemmascope_bilingual_activations). The latter will run on almost any relatively modern computer.\n:::\n\n## Introduction\n\nSparse autoencoders (SAEs) trained on the attention heads and residual streams of large language models have shown great promise at producing seemingly interpretable features [@cunningham2023sparse]. Features gathered from SAEs can be used to understand the inner workings of large language models and even to steer their behaviour in a desired direction [@templeton2024scaling]. \n\nIs not uncommon to find that some of the features learned by SAEs are multilingual, this is particularly interesting because it suggests that the model has learned to represent and reason through concepts in an abstract way that is independent of the language it is written in. The multilinguality of features, can be viewed as evidence for the *universality of features hypothesis*, which states that learned representations are universal and can form across models and tasks. This is one of the main speculative claims of the mechanistic interpretability agenda [@olah2020zoom].\n\nBut, how can we find these multilingual features in a SAE?\n\nMuch of the recent work regarding SAEs and mechanistic interpretability, has been about either scaling up the models to make them more powerful [@templeton2024scaling] [@gao2024scaling], finding techniques to make the models better at reconstructing the input [@rajamanoharan2024jumping], or using the learned features to find interesting circuits in the model [@wang2022interpretability]. Many of such endevours end up always finding *some* multilingual features, but they are not the main focus of the work, nor are they systematically searched for.\n\n\n\nIn this work, we present a systematic, data-driven process to generate a list of candidate bilingual features from a GemmaScope SAE. We define a bilingual interpretability score for each feature, which is dependent on a dataset of equivalent English-Spanish sentences. We then rank the features based on this score and analyze them. Finally, we discuss the potential for extending this methodology to include more that 2 languages.\n\n\n\n\n\n\n\n\n\n\n<!-- ## Hopes for Mechanistic Interpretability -->\n\n## Preliminaries: Quick Background on Sparse autoencoders\n\n### Basic Theory\n\n### JumpRelu Sparse autoencoder\n\n### GemmaScope SAEs\n\n## Methodology\n\n<!-- ### Overview of GemmaScope SAEs -->\n\n### Setup and Code\n\n### Results\n\n## Discussion\n\n## Conclusion\n\n## Apendix\n\n### On SAELens and TransformersLens\n\n### Interpretation of Feature Dashboards\n\n::: {#263b975c .cell .column-page execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nfrom IPython.display import IFrame\n\n# get a random feature from the SAE\nfeature_idx = 2647\n\nhtml_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n\ndef get_dashboard_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=0):\n    return html_template.format(sae_release, sae_id, feature_idx)\n\n# Neuronpedia names are usually different to the ones in HF/SAE_Lens\nneuronpedia_sae, neuronpedia_id = \"gemma-2-2b/20-gemmascope-res-16k\".split(\"/\")\nhtml = get_dashboard_html(sae_release = neuronpedia_sae, sae_id=neuronpedia_id, feature_idx=feature_idx)\nIFrame(html, width=1200, height=600)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n\n        <iframe\n            width=\"1200\"\n            height=\"600\"\n            src=\"https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/2647?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        \n```\n:::\n:::\n\n\n...\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
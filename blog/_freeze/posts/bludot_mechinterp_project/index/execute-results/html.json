{
  "hash": "2a326ffed9e33c93a616f98110f9903b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A Method to find Bilingual Features in Sparse autoencoders\"\ndescription: \"A systematic, data driven process to find Bilingual features inside GemmaScope Sparse autoencoder models.\"\nauthor: \n    name: \"Diego Andrés Gómez Polo\"\n    email: \"diego.polo@rappi.com\"\n    affiliations:\n        - name: \"Rappi\"\ndate: \"9/29/2024\"\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show source\"\n    code-copy: true\n    smooth-scroll: true\n# bibliography: ref.bib\n# csl: ieee-with-url.csl\n# title-block-banner: images/banner.png\n# title-block-banner-color: black\ncategories: [\"en\", \"NLP\", \"Mechanistic Interpretability\", \"SAE\", \"Multilinguality\", \"Research Style Blog\"]\n# image: images/thumbnail.png\ntoc: true\ntoc-location: left\ncomments: \n  utterances:\n    repo: diegommezp28/playground\n    label: blog-comments\n    theme: github-dark-orange\n    issue-term: pathname\nbibliography: ref.bib\nwebsite:\n  # open-graph: \n  #   image: images/thumbnail.png\n  twitter-card: true\neditor:\n  render-on-save: true\n# jupyter: python3\n---\n\n::: column-margin\n## Reproducibility\nTo reproduce all the results, feel free to use this [Colab Notebook](https://colab.research.google.com). But, be aware that in order to run the part of the code that gathers the activations, you will need around 24-25GB of RAM in CPU or close to that VRAM if on GPU. The colab free tier does not provide this amount of resources. You can still run the analysis part of the code with this [dataset](https://huggingface.co/datasets/diegomezp/gemmascope_bilingual_activations). The latter will run on almost any relatively modern computer.\n:::\n\n## Introduction\n\nSparse autoencoders (SAEs) trained on the attention heads and residual streams of large language models have shown great promise at producing seemingly interpretable features [@cunningham2023sparse]. Features gathered from SAEs can be used to understand the inner workings of large language models and even to steer their behaviour in a desired direction [@templeton2024scaling]. \n\nIs not uncommon to find that some of the features learned by SAEs are multilingual, this is particularly interesting because it suggests that the model has learned to represent and reason through concepts in an abstract way that is independent of the language it is written in. The multilinguality of features, can be viewed as evidence for the *universality of features hypothesis*, which states that learned representations are universal and can form across models and tasks. This is one of the main speculative claims of the mechanistic interpretability agenda [@olah2020zoom].\n\nBut, how can we find these multilingual features in a SAE?\n\nMuch of the recent work regarding SAEs and mechanistic interpretability, has been about either scaling up the models to make them more powerful [@templeton2024scaling] [@gao2024scaling], finding techniques to make the models better at reconstructing the input [@rajamanoharan2024jumping], or using the learned features to find interesting circuits in the model [@wang2022interpretability]. Many of such endevours end up always finding *some* multilingual features, but they are not the main focus of the work, nor are they systematically searched for.\n\n\n\nIn this work, we present a systematic, data-driven process to generate a list of candidate bilingual features from a GemmaScope SAE. We define a bilingual interpretability score for each feature, which is dependent on a dataset of equivalent English-Spanish sentences. We then rank the features based on this score and analyze them. Finally, we discuss the potential for extending this methodology to include more that 2 languages.\n\n\n\n\n\n\n\n\n\n\n<!-- ## Hopes for Mechanistic Interpretability -->\n\n## Preliminaries: Quick Background on Sparse autoencoders\n\n### Basic Theory\n\n### JumpRelu Sparse autoencoder\n\n### GemmaScope SAEs\n\n## Methodology\n\n<!-- In this section, we will describe the methodology used to find bilingual features in a given SAE. First, we will describe the data collection process, then we will explain how we extract features from the SAE, and finally, we will describe the bilingual interpretability score we use to rank the features. -->\n\n**The driving intuition behind our methodology** is that, inspite of changes in tokenization, word order, general linguistic structure, and even the distribution of feature logits across languages, for a feature to be bilingual, it is necessary that it circumvents these differences and be activated by the same or similar sentences in both languages.\n\nSuch condition may not be sufficient, but as we will see, it is a good starting point to find bilingual features in a SAE.\n\nIn this section, we will describe the specific methodology that arises from this intuition, which consists of three main steps:\n\n1. **Data Collection**: We gather a dataset of equivalent English-Spanish sentences.\n2. **Feature Extraction**: We extract the features from the SAE for each sentence in the dataset.\n3. **Bilingual Interpretability Score**: We define a score that measures how similar the activations of a feature are across languages.\n\n### Basic Setup\n\nFor our experiments, we used ***Gemma 2-2B*** as our language model on its base pretrained version without any intruction tuning. We focused our experiments on a single SAE from the GemmaScope collection of open-source SAEs [@lieberum2024gemma], specifically, the one with id *gemma-scope-2b-pt-res-canonical/layer_20/width_16k/canonical*. This SAE has 16k features and is trained on the residual stream of the 20th layer of the model. It is the smallest version of this particular hook point, and the choice for its size was made purely for computational reasons.\n\nThe 2B version of the Gemma 2 models has 26 layers [@team2024gemma], so a SAE trained on the 20th residual stream is expected to have learned more abstract features than earlier layers. Moreover, we decided to use the residual stream instead of the attention heads because it is an information bottleneck where not only the prior attention head writes to, but also all the later ones, so one should expect that the features learned in this specific point are more abstract and general than those inside the attention mechanism [@elhage2021mathematical].\n\nFor our bilingual dataset, we will use a small sample of the OPUS Books dataset [@tiedemann-2012-parallel], with equivalent English-Spanish sentences. \n\n\n### Feature Extraction\n\nTo extract the features from the SAE, we used the `HookedSAETransformer` class from the `SAELens` library. This class allows us to *hook* our SAE to a given language model, and cache the activations of the SAE for a given input. \n\n<!-- ### Overview of GemmaScope SAEs -->\n\n### Billingual Interpretability Score\n\n### Results\n\n## Discussion\n\n## Conclusion\n\n## Apendix\n\n### On SAELens and TransformersLens\n\n### Interpretation of Feature Dashboards\n\n::: {#ce5185e2 .cell .column-page execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nfrom IPython.display import IFrame\n\n# get a random feature from the SAE\nfeature_idx = 2647\n\nhtml_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n\ndef get_dashboard_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=0):\n    return html_template.format(sae_release, sae_id, feature_idx)\n\n# Neuronpedia names are usually different to the ones in HF/SAE_Lens\nneuronpedia_sae, neuronpedia_id = \"gemma-2-2b/20-gemmascope-res-16k\".split(\"/\")\nhtml = get_dashboard_html(sae_release = neuronpedia_sae, sae_id=neuronpedia_id, feature_idx=feature_idx)\nIFrame(html, width=1200, height=600)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n\n        <iframe\n            width=\"1200\"\n            height=\"600\"\n            src=\"https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/2647?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        \n```\n:::\n:::\n\n\n...\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}